{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task2P3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPb1G8rR+s2SGGm+xsjf+LL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sohaib90/Pytorch-projects-uni-random-/blob/master/TextClassificationP3T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI4_W0iqq39Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install https://github.com/pytorch/text/archive/master.zip\n",
        "!pip install inscriptis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CWQx-cPs5rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch    # root package\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tUO-irrs-2d",
        "colab_type": "code",
        "outputId": "29e39261-2647-40cb-fdee-052e1654a19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "\"\"\"\n",
        "Load the AG_NEWS dataset in bi-gram features format.\n",
        "\"\"\"\n",
        "# !pip install https://github.com/pytorch/text/archive/master.zip\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import text_classification\n",
        "import os\n",
        "\n",
        "NGRAMS = 2\n",
        "\n",
        "if not os.path.isdir('./.data'):\n",
        "    os.mkdir('./.data')\n",
        "\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ag_news_csv.tar.gz: 11.8MB [00:00, 29.6MB/s]\n",
            "120000lines [00:07, 15732.43lines/s]\n",
            "120000lines [00:14, 8124.94lines/s]\n",
            "7600lines [00:00, 8577.02lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJpvo3Ta86vL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_dataset.get_labels()))\n",
        "print(test_dataset[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJCK81PXtECf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U5hs8NJtQ4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMmodel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_dim, dim_embed, num_classes): \n",
        "    super(LSTMmodel, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embeddings = nn.EmbeddingBag(vocab_size, dim_embed)\n",
        "\n",
        "    self.lstm = nn.LSTM(dim_embed, hidden_dim)\n",
        "    self.linear = nn.Linear(hidden_dim, num_classes)\n",
        "    # one hot encoded vectors\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    self.initialize_weights()\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    num_range = 0.5\n",
        "    self.embeddings.weight.data.uniform_(-num_range, num_range)\n",
        "    self.linear.weight.data.uniform_(-num_range, num_range)\n",
        "    self.linear.bias.data.zero_()\n",
        "\n",
        "\n",
        "  def forward(self, x, offset):\n",
        "    embedding = self.embeddings(x,offset)\n",
        "    lstm_out, _ = self.lstm(embedding.view(len(embedding),1,-1))\n",
        "    out = self.linear(lstm_out.view(len(lstm_out),-1))\n",
        "    return self.softmax(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr9_JuOoyDRc",
        "colab_type": "code",
        "outputId": "e1316164-2871-451c-9c0f-4308ba1b5972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "HIDDEN_DIM = 16\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "\n",
        "model = LSTMmodel(VOCAB_SIZE, HIDDEN_DIM, EMBED_DIM, NUM_CLASS)\n",
        "model.cuda()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (embeddings): EmbeddingBag(1308844, 32, mode=mean)\n",
              "  (lstm): LSTM(32, 16)\n",
              "  (linear): Linear(in_features=16, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqS0WwYRMukV",
        "colab_type": "code",
        "outputId": "d1c931e3-2fc1-4f50-8938-8f4e8126b4f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print (name, param.requires_grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings.weight True\n",
            "lstm.weight_ih_l0 True\n",
            "lstm.weight_hh_l0 True\n",
            "lstm.bias_ih_l0 True\n",
            "lstm.bias_hh_l0 True\n",
            "linear.weight True\n",
            "linear.bias True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zznYrd7Iyj76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(batch):\n",
        "    \n",
        "    label = torch.tensor([entry[0] for entry in batch])\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text = torch.cat(text)\n",
        "    \n",
        "    return text, offsets, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ho5AyEY8G6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_labels (x):\n",
        "  labels = np.zeros((x.size()[0], len(train_dataset.get_labels())))\n",
        "  for val in range(x.size()[0]):\n",
        "     labels[val][x[val]] = 1\n",
        "     \n",
        "  return torch.from_numpy(labels).type(torch.FloatTensor)\n",
        "\n",
        "# print(convert_labels(torch.from_numpy(np.asarray([1,2,0,2,0,1,2,3,3,2,1])).type(torch.LongTensor)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5U9nzDOzAG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_data):\n",
        "\n",
        "    # Initial values of training loss and training accuracy\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    # TODO: Use the PyTorch DataLoader class to load the data \n",
        "    # into shuffled batches of appropriate sizes into the variable 'data'.\n",
        "    # Remember, this is the place where you need to generate batches.\n",
        "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "    \n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        \n",
        "        # TODO: What do you need to do in order to perform backprop on the optimizer?\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        cls = convert_labels(cls).to(device)\n",
        "\n",
        "        # TODO: Store the output of the model in variable 'output'\n",
        "        output = model(text, offsets)\n",
        "        \n",
        "        # TODO: Define the 'loss' variable (with respect to 'output' and 'cls').\n",
        "        # Also calculate the total loss in variable 'train_loss'\n",
        "        loss = criterion(output, cls)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # TODO: Perform the backward propagation on 'loss' and \n",
        "        # optimize it through the 'optimizer' step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        # TODO: Calculate and store the total training accuracy\n",
        "        # in the variable 'total_acc'.\n",
        "        # Remember, you need to find the \n",
        "        train_acc = train_acc + (output.argmax(1) == cls.argmax(1)).sum().item()\n",
        "        \n",
        "\n",
        "    # TODO: Adjust the learning rate here using the scheduler step\n",
        "    scheduler.step()\n",
        "    \n",
        "    \n",
        "\n",
        "    return train_loss / len(train_data), train_acc / len (train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDgn_vmszEB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_data):\n",
        "    \n",
        "    # Initial values of test loss and test accuracy\n",
        "    \n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    # TODO: Use DataLoader class to load the data\n",
        "    # into non-shuffled batches of appropriate sizes.\n",
        "    # Remember, you need to generate batches here too.\n",
        "    data = DataLoader(test_data, batch_size = BATCH_SIZE, collate_fn = generate_batch)\n",
        "    \n",
        "    \n",
        "    for text, offsets, cls in data:\n",
        "        \n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        cls = convert_labels(cls).to(device)\n",
        "        \n",
        "        # Hint: There is a 'hidden hint' here. Let's see if you can find it :)\n",
        "        with torch.no_grad():# because during test we dont need to compute the grdient of any tensor\n",
        "\n",
        "            \n",
        "            # TODO: Get the model output\n",
        "          output = model(text, offsets)  \n",
        "            \n",
        "            \n",
        "            # TODO: Calculate and add the loss to find total 'loss'\n",
        "          loss = criterion(output, cls)\n",
        "          loss += loss.item()\n",
        "            \n",
        "            \n",
        "            # TODO: Calculate the accuracy and store it in the 'acc' variable\n",
        "          acc += (output.argmax(1) == cls.argmax(1)).sum().item()\n",
        "            \n",
        "            \n",
        "\n",
        "    return loss / len(test_data), acc / len(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhOli65ICQLF",
        "colab_type": "code",
        "outputId": "bc66c2f4-094d-4e43-855a-eeff6cf150a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZhC_wvJutZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/weights_NN/lstm_model_softmax_v2(92.1%).pt'))\n",
        "for name, param in model.named_parameters():\n",
        "  if \"embedding\" in name:\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks2TdzRDMgXo",
        "colab_type": "code",
        "outputId": "c4eccf0e-d8bb-4716-f20f-9865e9d9e670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print (name, param.requires_grad)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings.weight False\n",
            "lstm.weight_ih_l0 True\n",
            "lstm.weight_hh_l0 True\n",
            "lstm.bias_ih_l0 True\n",
            "lstm.bias_hh_l0 True\n",
            "linear.weight True\n",
            "linear.bias True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRawXcK95cH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# TODO: Set the number of epochs and the learning rate to \n",
        "# their initial values here\n",
        "\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "TRAIN_RATIO = 0.9\n",
        "\n",
        "# TODO: Set the intial validation loss to positive infinity\n",
        "valid_loss = float('inf')\n",
        "\n",
        "# TODO: Use the appropriate loss function\n",
        "criterion = torch.nn.BCELoss().to(device) # because classification \n",
        "\n",
        "# TODO: Use the appropriate optimization algorithm with parameters (Suggested: SGD)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE, weight_decay=1e-7)\n",
        "\n",
        "# TODO: Use a scheduler function\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.6)\n",
        "valid_acc_dummy = 1e-3\n",
        "\n",
        "# TODO: Split the data into train and validation sets using random_split()\n",
        "train_len = int(len(train_dataset) * TRAIN_RATIO)\n",
        "valid_len = len(train_dataset) - train_len\n",
        "train_split_dataset , valid_split_dataset = random_split(train_dataset, [train_len, valid_len])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QdRFl_HzHSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Code\n",
        "# TODO: Finish the rest of the code below\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(train_split_dataset)\n",
        "    valid_loss, valid_acc = test(valid_split_dataset)\n",
        "    if (valid_acc > valid_acc_dummy):\n",
        "      torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_NN/lstm_model_softmax_v3.pt')\n",
        "      valid_acc_dummy = valid_acc\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Lj-R1LzKy4",
        "colab_type": "code",
        "outputId": "adb37ead-bd60-4088-a383-d61fa7bc0754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "print('Building test model and loading the saved model...')\n",
        "\n",
        "VOCAB_SIZE_TEST = len(test_dataset.get_vocab())\n",
        "EMBED_DIM_TEST = 32\n",
        "HIDDEN_DIM_TEST = 16\n",
        "NUM_CLASS_TEST = len(test_dataset.get_labels())\n",
        "\n",
        "model = LSTMmodel(VOCAB_SIZE_TEST,HIDDEN_DIM_TEST,EMBED_DIM_TEST,NUM_CLASS_TEST)\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/weights_NN/lstm_model_softmax_v3.pt'))\n",
        "model.cuda().eval()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building test model and loading the saved model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (embeddings): EmbeddingBag(1308844, 32, mode=mean)\n",
              "  (lstm): LSTM(32, 16)\n",
              "  (linear): Linear(in_features=16, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5oEM3dt4-Ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e93004d3-d2c5-485d-946c-0afce638e7e4"
      },
      "source": [
        "# checking test model \n",
        "print (\"Checking the properties of the test model...\")\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "  # print(name, param.requires_grad)\n",
        "  # if param.requires_grad == True:\n",
        "    # raise NameError(\"Testing model is not in eval mode..\")  "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the properties of the test model...\n",
            "embeddings.weight True\n",
            "lstm.weight_ih_l0 True\n",
            "lstm.weight_hh_l0 True\n",
            "lstm.bias_ih_l0 True\n",
            "lstm.bias_hh_l0 True\n",
            "linear.weight True\n",
            "linear.bias True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjBF-qdC4wml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f17f61f3-64fe-450d-8587-587bcea18701"
      },
      "source": [
        "# Testing error calculation\n",
        "\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLoss: 0.0001(test)\t|\tAcc: 92.1%(test)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCMb0Z5NlmQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "from inscriptis import get_text\n",
        "\n",
        "def get_text_url(url_link):\n",
        "  html = urllib.request.urlopen(url_link).read().decode('utf-8')\n",
        "  return get_text(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkDlZzsKO5aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries\n",
        "\n",
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# labels for the AG_NEWS dataset\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67I4VdPqOmMx",
        "colab_type": "code",
        "outputId": "00a93b21-949d-4882-fc19-d8a1660d0827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "url_random_travel = \"https://www.nytimes.com/2020/01/22/sports/football/eli-manning-retirement.html\"\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(get_text_url(url_random_travel), model, vocab, NGRAMS)])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sports' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHX-8VRhPDRk",
        "colab_type": "code",
        "outputId": "1d0cd745-08a6-4a77-c846-cc7f7570ed4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text_test = \"Amazon, Alphabet, Alibaba, Facebook, Tencent - five of the world's 10 most valuable companies,\\\n",
        "all less than 25 years old - and all got rich, in their own ways, on data.No wonder it's become common to call data the 'new oil'.\\\n",
        "As recently as 2011, five of the top 10 were oil companies. Now, only ExxonMobil clings on.\\\n",
        "The analogy isn't perfect. Data can be used many times, oil only once.\\\n",
        "But data is like oil in that the crude, unrefined stuff is not much use to anyone.\\\n",
        "You have to process it to get something valuable. fortnightYou refine oil to make diesel, to put it in an engine.\\\n",
        "With data, you need to analyse it to provide insights that can inform decisions - which advert to insert in a social media timeline, which search result to put at the top of the page.\\\n",
        "Imagine you were asked to make just one of those decisions.\\\n",
        "Someone is watching a video on YouTube, which is run by Google, which is owned by Alphabet. What should the system suggest they watch next?\\\n",
        " Pique their interest, and YouTube gets to serve them another advert. Lose their attention, and they will click away.\\\n",
        "You have all the data you need. Consider every other YouTube video they have ever watched - what are they interested in? Now, look at what other users have gone on to watch after this video.\\\n",
        "Weigh up the options, calculate probabilities. If you choose wisely, and they view another ad, well done - you've earned Alphabet all of, ooh, maybe 20 cents (15p).\\\n",
        "Clearly, relying on humans to process data would be impossibly inefficient. These business models need machines.\\\n",
        "In the data economy, power comes not from data alone but from the interplay of data and algorithm.\"\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(text_test, model, vocab, NGRAMS)])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Business' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmlzCaiDQ7E6",
        "colab_type": "code",
        "outputId": "a460a477-ec5a-44c5-9bf1-a9ea29dcc4d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_text2 = \"Demand for iPhones appears to be flourishing once again in China, a year after Apple had to warn investors that the Chinese market was facing a serious slow down.\\\n",
        "IPhone sales in China were up 18% in December from the same month a year earlier, an even better performance than Wall Street had projected, according to an investor note from Wedbush analyst Dan Ives. Apple (AAPL) shipped around 3.2 million iPhones to China during the month compared to 2.7 million in December 2018, Ives reported, citing data from the China Academy of Information and Communication Technology.\\\n",
        "It's good news for Apple, after iPhone sales tumbled in China over the past year.\\\n",
        "'Our belief that China will continue this positive upward trajectory with renewed growth and share gains on the heels of an iPhone 11 product cycle which the skeptics continue to underestimate,' Ives said in the Thursday note.\\\n",
        "The good news was reflected in Apple's stock, which was up nearly 2% to a record high on Thursday.\\\n",
        "China is a key market for Apple — the region makes up nearly 17% of the company's total sales. And the iPhone is Apple's biggest profit driver.\\\n",
        "In early January 2019, Apple CEO Tim Cook wrote a letter to investors warning them to expect lower sales from the holiday quarter due primarily to iPhone sales in China falling short of what the company had expected. It was the first time since June 2002 that Apple issued a reduction in its quarterly revenue forecast. When the company reported earnings for that quarter later in January, iPhone sales had fallen 15% from the prior year.\\\n",
        "A number of factors contributed to the drop, namely slower growth in the Chinese economy and the US-China trade war.\\\n",
        "The trend continued throughout much of last year.\\\n",
        "In April, Apple said its iPhone sales in the first three months of 2019 dropped 17% from the same period a year earlier, again because of sluggish demand in China. A month later, Citi analysts warned that the trade war could cause Apple's iPhone sales in China to be cut in half.\\\n",
        "In the three months ending in June 2019, iPhones made up less than half of the company's revenue for the first time in years, though the slump also coincided with a greater focus at Apple on subscription-based services such as Apple Music.\\\n",
        "But the iPhone 11, which Apple introduced in September with better camera technology and battery life, as well as lower-than-expected prices, has helped with the rebound, Ives said. Early demand for the new model was strong, and in Apple's October earnings call, Cook noted that the company's prospects in China were turning around.\\\n",
        "Now, Ives estimates that there are roughly 60 million to 70 million iPhone users in China who are likely to upgrade their phones in the coming months.\\\n",
        "The momentum probably will continue this year, as Apple analysts widely expect Apple to release a 5G-enabled version of the iPhone in the fall.\\\n",
        "'Many investors are asking us: Is all the good news baked into shares after an historic upward move over the last year?' Ives said in the note. 'The answer from our vantage point is a resounding NO, as we view [this as] only the first part of this massive upgrade opportunity.\"\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(test_text2, model, vocab, NGRAMS)])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5LVAz9YRaVP",
        "colab_type": "code",
        "outputId": "429ff262-7324-4fd1-e0cb-607ea92c221e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fortnight = \"Microsoft on Wednesday said it conducted an investigation into a security breach of one of its customer databases and found records could have been exposed for\\\n",
        "a short period in December. A misconfiguration in a database's Azure security rules on Dec. 5 enabled exposure to millions of customer support records, according to a blog post from Microsoft on Wednesday. After being alerted of the issue, \\\n",
        "engineers fixed the problem as of Dec. 31. The company says there was no malicious use of the data but is disclosing the breach to be transparent to its customers.\\\n",
        "Misconfigurations are unfortunately a common error across the industry, the company said.\\\n",
        "We have solutions to help prevent this kind of mistake, but unfortunately, they were not enabled for this database. As we've learned, it is good to periodically review your own configurations and ensure you are taking advantage of all protections available.\\\n",
        "Most customer data stored in the databases had personal information redacted, Microsoft said. The company said it'll contact customers whose info may have not been redacted.\\\n",
        "Bob Diachenko, a security researcher with Comparitech, discovered the security lapse on Dec. 28. He alerted Microsoft about the issue on Dec. 29 leading to the fix two days later. \"\n",
        "\n",
        "print(\"This is a '%s' news\" % ag_news_label[predict(fortnight, model, vocab, NGRAMS)])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a 'Sci/Tec' news\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}